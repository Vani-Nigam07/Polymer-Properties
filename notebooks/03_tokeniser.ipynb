{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d003e8",
   "metadata": {},
   "source": [
    "## LSTM is better for small datsets and I have Nan values too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f2ca6",
   "metadata": {},
   "source": [
    "| Step                        | Purpose                                                         |\n",
    "| --------------------------- | --------------------------------------------------------------- |\n",
    "| 1. **Tokenization**         | Convert SMILES string into a sequence of characters or subwords |\n",
    "| 2. **Embedding Layer**      | Map each token to a dense vector                                |\n",
    "| 3. **LSTM Encoder**         | Learn sequential patterns of the molecule                       |\n",
    "| 4. **Pooling / Projection** | Reduce sequence output to a fixed-size embedding                |\n",
    "| 5. **Save/Use Embedding**   | Use for downstream models (e.g., hybrid model, fusion, MLP)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb29df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "#from rdkit.Chem.rdFingerprintGenerator import MorganFingerprintGenerator\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c34555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import rdkit.Chem.rdFingerprintGenerator as rdFingerprintGenerator\n",
    "\n",
    "#from rdkit.Chem.rdFingerprintGenerator import MorganFingerprintGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5134e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ed45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Path to your config file\n",
    "CONFIG_PATH = r\"D:\\Skills\\new\\NeurIPS2\\config.yaml\"\n",
    "\n",
    "# Load it\n",
    "with open(CONFIG_PATH, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fd930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffv_merged = config['output']['ffv_merged_csv']\n",
    "Tc_cleaned = config['output']['Tc_csv']\n",
    "Tg_cleaned = config['output']['Tg_csv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aeda3",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189efaf",
   "metadata": {},
   "source": [
    "SMILES are not like natural language — special characters (=, #, Cl, Br, (), []) carry precise chemical meaning. So, character-based or chem-aware tokenizers are used instead of NLP-style ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa40ea",
   "metadata": {},
   "source": [
    "# SELFIES (SELF-referencIng Embedded Strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4172393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53caeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smiles_to_selfies(smiles):\n",
    "    try:\n",
    "        return sf.encoder(smiles)\n",
    "    except:\n",
    "        return None  # Invalid SMILES or conversion failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77fe00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_selfies(selfie_str):\n",
    "    if selfie_str is None:\n",
    "        return ['[UNK]']\n",
    "    return sf.split_selfies(selfie_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646bcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_selfies_vocab(smiles_list):\n",
    "    tokens = set()\n",
    "    for smi in smiles_list:\n",
    "        selfie = smiles_to_selfies(smi)\n",
    "        tokens.update(tokenize_selfies(selfie))\n",
    "    tokens = sorted(list(tokens))\n",
    "    token_to_idx = {tok: i+2 for i, tok in enumerate(tokens)}\n",
    "    token_to_idx['<PAD>'] = 0\n",
    "    token_to_idx['<UNK>'] = 1\n",
    "    return token_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92d4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_selfies(smi, token_to_idx, max_len=150):\n",
    "    selfie = smiles_to_selfies(smi)\n",
    "    tokens = tokenize_selfies(selfie)\n",
    "    indices = [token_to_idx.get(tok, token_to_idx['<UNK>']) for tok in tokens]\n",
    "\n",
    "    # Pad or truncate\n",
    "    if len(indices) < max_len:\n",
    "        indices += [token_to_idx['<PAD>']] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def selfies_list_to_tensor(smiles_list, token_to_idx, max_len=150):\n",
    "    encoded = [encode_selfies(smi, token_to_idx, max_len) for smi in smiles_list]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffv_merged_df = pd.read_csv(ffv_merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c94a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ffv_df = ffv_merged_df.copy()  # or whatever your DataFrame is named\n",
    "ffv_df['SMILES'] = ffv_df['SMILES'].str.strip()  # clean whitespace\n",
    "smiles_list = ffv_df['SMILES'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de62e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = build_selfies_vocab(smiles_list)\n",
    "padded_selfies_tensor = selfies_list_to_tensor(smiles_list, token_to_idx, max_len=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847a2103",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_selfies_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpadded_selfies_tensor\u001b[49m[\u001b[38;5;241m118\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'padded_selfies_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "padded_selfies_tensor[118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4580486",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfies = config['output']['seflies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fcd46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion failed for *OC(=O)CCCCCCCCC(=O)OC1COC2C(*)COC12: failed to parse input\n",
      "\tSMILES: *OC(=O)CCCCCCCCC(=O)OC1COC2C(*)COC12\n",
      "Conversion failed for *Nc1ccc([C@H]2[C@@H]3C[C@H]4C[C@@H](C3)C[C@@H]2C4)cc1N*: failed to parse input\n",
      "\tSMILES: *Nc1ccc([C@H]2[C@@H]3C[C@H]4C[C@@H](C3)C[C@@H]2C4)cc1N*\n",
      "Conversion failed for *C(=O)Nc1ccc(Oc2ccc(Oc3ccc(NC(=O)c4ccc5c(c4)C(=O)N(c4ccc(Oc6ccc(C7(c8ccc(Oc9ccc(N%10C(=O)c%11ccc(*)cc%11C%10=O)cc9)cc8)CC8CC7C7CCCC87)cc6)cc4)C5=O)cc3)c(C(C)(C)C)c2)cc1: failed to parse input\n",
      "\tSMILES: *C(=O)Nc1ccc(Oc2ccc(Oc3ccc(NC(=O)c4ccc5c(c4)C(=O)N(c4ccc(Oc6ccc(C7(c8ccc(Oc9ccc(N%10C(=O)c%11ccc(*)cc%11C%10=O)cc9)cc8)CC8CC7C7CCCC87)cc6)cc4)C5=O)cc3)c(C(C)(C)C)c2)cc1\n",
      "Conversion failed for *CC(*)(C)C(=O)OCCCCCCCCCOc1ccc2cc(C(=O)Oc3ccccc3)ccc2c1: failed to parse input\n",
      "\tSMILES: *CC(*)(C)C(=O)OCCCCCCCCCOc1ccc2cc(C(=O)Oc3ccccc3)ccc2c1\n",
      "Conversion failed for *Nc1ccc(-c2ccc(-c3ccc(N*)cc3)cc2)cc1: failed to parse input\n",
      "\tSMILES: *Nc1ccc(-c2ccc(-c3ccc(N*)cc3)cc2)cc1\n"
     ]
    }
   ],
   "source": [
    "for smi in smiles_list[5:10]:\n",
    "    try:\n",
    "        selfies_str = sf.encoder(smi)\n",
    "        print(\"SMILES:\", smi, \"→ SELFIES:\", selfies_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion failed for {smi}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb468417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RdkitFingerprintDataset(Dataset):\n",
    "    def __init__(self, pt_path):\n",
    "        data = torch.load(pt_path)\n",
    "        self.fingerprints = data[\"fingerprints\"]\n",
    "        self.smiles = data[\"smiles\"]\n",
    "        self.ids = data[\"id\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fingerprints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"fingerprint\": self.fingerprints[idx],\n",
    "            \"smiles\": self.smiles[idx],\n",
    "            \"id\": self.ids[idx]\n",
    "        }\n",
    "\n",
    "# Usage:\n",
    "dataset = RdkitFingerprintDataset(\"rdkit_ffv.pt\")\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676ff280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both tensor and vocab in one file\n",
    "torch.save({\n",
    "    'input_tensor': padded_selfies_tensor,  # shape: [N, 150]\n",
    "    'token_to_idx': token_to_idx,           # dict mapping SELFIES tokens to IDs\n",
    "    'smiles': smiles_list                   # to track or map back\n",
    "}, \"selfies.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea00769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbebc21a",
   "metadata": {},
   "source": [
    "## LSTM Encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymer-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
